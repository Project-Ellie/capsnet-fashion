{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0-rc1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import mnist_tools\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t10k-images-idx3-ubyte.gz',\n",
       " 't10k-labels-idx1-ubyte.gz',\n",
       " 'train-labels-idx1-ubyte.gz',\n",
       " 'train-images-idx3-ubyte.gz']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"/var/ellie/data/mnist\"\n",
    "img_train = os.path.join(DATA_DIR, \"train-images-idx3-ubyte.gz\")\n",
    "lbl_train = os.path.join(DATA_DIR, \"train-labels-idx1-ubyte.gz\")\n",
    "img_test = os.path.join(DATA_DIR, \"t10k-images-idx3-ubyte.gz\")\n",
    "lbl_test = os.path.join(DATA_DIR, \"t10k-labels-idx1-ubyte.gz\")\n",
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scale(imgs, lbls):\n",
    "    return imgs / 256, lbls\n",
    "\n",
    "BATCH_SIZE=1000\n",
    "tr, te = mnist_tools.datasets(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext_1:0' shape=(?, 784) dtype=uint8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = tr.repeat().batch(BATCH_SIZE).make_one_shot_iterator().get_next()\n",
    "tr_images, tr_labels = train_iter\n",
    "tr_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1 154 253  90   0   0   0   0   0   0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    i,l = sess.run([tr_images, tr_labels])\n",
    "    print(i[0, 290:300])\n",
    "i.shape, l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dense( hidden ):\n",
    "    def _dense ( X, Y_ ):\n",
    "        #X=tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "        #Y_=tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "        X = tf.cast(X, tf.float32)\n",
    "        X = tf.subtract(tf.divide(X, tf.constant(256.0)), tf.constant(0.5))\n",
    "        W1 = tf.Variable(tf.random_normal(shape=[784, hidden]), dtype=tf.float32)\n",
    "        b1 = tf.Variable(tf.random_normal(shape=[hidden]), dtype=tf.float32)\n",
    "        a1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "        W2 = tf.Variable(tf.random_normal(shape=[hidden, 10]), dtype=tf.float32)\n",
    "        b2 = tf.Variable(tf.random_normal(shape=[10]), dtype=tf.float32)\n",
    "        logits = tf.matmul(a1, W2) + b2\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y_, logits=logits))\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        accuracy=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), pred), dtype=tf.float32))\n",
    "        return (X, Y_, train_op, loss, accuracy)\n",
    "    return _dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y_, train_op, loss, accuracy = Dense(128)(tr_images, tr_labels)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of epochs  : 100 \n",
      "batch size        : 1000 \n",
      "report every after: 100 steps\n",
      "Batch No 0: accuracy: 0.13, loss: 141.9151\n",
      "Batch No 100: accuracy: 0.532, loss: 16.102262\n",
      "Batch No 200: accuracy: 0.679, loss: 9.837561\n",
      "Batch No 300: accuracy: 0.769, loss: 5.5614495\n",
      "Batch No 400: accuracy: 0.791, loss: 4.070766\n",
      "Batch No 500: accuracy: 0.799, loss: 4.790814\n",
      "Batch No 600: accuracy: 0.853, loss: 3.1983514\n",
      "Batch No 700: accuracy: 0.863, loss: 2.29559\n",
      "Batch No 800: accuracy: 0.844, loss: 3.3503318\n",
      "Batch No 900: accuracy: 0.884, loss: 2.3239386\n",
      "Batch No 1000: accuracy: 0.886, loss: 1.5484669\n",
      "Batch No 1100: accuracy: 0.869, loss: 2.5623999\n",
      "Batch No 1200: accuracy: 0.902, loss: 1.8710258\n",
      "Batch No 1300: accuracy: 0.903, loss: 1.1629597\n",
      "Batch No 1400: accuracy: 0.887, loss: 2.0413601\n",
      "Batch No 1500: accuracy: 0.911, loss: 1.5472491\n",
      "Batch No 1600: accuracy: 0.916, loss: 0.9090353\n",
      "Batch No 1700: accuracy: 0.913, loss: 1.6893337\n",
      "Batch No 1800: accuracy: 0.922, loss: 1.2785798\n",
      "Batch No 1900: accuracy: 0.929, loss: 0.7338591\n",
      "Batch No 2000: accuracy: 0.917, loss: 1.4365108\n",
      "Batch No 2100: accuracy: 0.93, loss: 1.0597824\n",
      "Batch No 2200: accuracy: 0.937, loss: 0.61034393\n",
      "Batch No 2300: accuracy: 0.922, loss: 1.2414402\n",
      "Batch No 2400: accuracy: 0.94, loss: 0.91747\n",
      "Batch No 2500: accuracy: 0.948, loss: 0.5233468\n",
      "Batch No 2600: accuracy: 0.928, loss: 1.0756077\n",
      "Batch No 2700: accuracy: 0.939, loss: 0.8023346\n",
      "Batch No 2800: accuracy: 0.95, loss: 0.45293558\n",
      "Batch No 2900: accuracy: 0.936, loss: 0.94178313\n",
      "Batch No 3000: accuracy: 0.945, loss: 0.7105723\n",
      "Batch No 3100: accuracy: 0.95, loss: 0.39573035\n",
      "Batch No 3200: accuracy: 0.939, loss: 0.8185198\n",
      "Batch No 3300: accuracy: 0.951, loss: 0.6271079\n",
      "Batch No 3400: accuracy: 0.958, loss: 0.3511467\n",
      "Batch No 3500: accuracy: 0.941, loss: 0.69953\n",
      "Batch No 3600: accuracy: 0.954, loss: 0.5566476\n",
      "Batch No 3700: accuracy: 0.96, loss: 0.32048813\n",
      "Batch No 3800: accuracy: 0.95, loss: 0.5960345\n",
      "Batch No 3900: accuracy: 0.959, loss: 0.4910201\n",
      "Batch No 4000: accuracy: 0.965, loss: 0.28297004\n",
      "Batch No 4100: accuracy: 0.951, loss: 0.52236176\n",
      "Batch No 4200: accuracy: 0.963, loss: 0.43923816\n",
      "Batch No 4300: accuracy: 0.971, loss: 0.25395322\n",
      "Batch No 4400: accuracy: 0.956, loss: 0.45101148\n",
      "Batch No 4500: accuracy: 0.963, loss: 0.39531463\n",
      "Batch No 4600: accuracy: 0.974, loss: 0.23088348\n",
      "Batch No 4700: accuracy: 0.96, loss: 0.39609247\n",
      "Batch No 4800: accuracy: 0.967, loss: 0.3499998\n",
      "Batch No 4900: accuracy: 0.975, loss: 0.21001482\n",
      "Batch No 5000: accuracy: 0.96, loss: 0.33524644\n",
      "Batch No 5100: accuracy: 0.973, loss: 0.31118423\n",
      "Batch No 5200: accuracy: 0.976, loss: 0.1874254\n",
      "Batch No 5300: accuracy: 0.965, loss: 0.28341347\n",
      "Batch No 5400: accuracy: 0.976, loss: 0.2778769\n",
      "Batch No 5500: accuracy: 0.979, loss: 0.16661936\n",
      "Batch No 5600: accuracy: 0.971, loss: 0.25217062\n",
      "Batch No 5700: accuracy: 0.973, loss: 0.25694308\n",
      "Batch No 5800: accuracy: 0.981, loss: 0.1561788\n",
      "Batch No 5900: accuracy: 0.976, loss: 0.21569292\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 100\n",
    "number_of_steps = number_of_epochs * 60000 // BATCH_SIZE\n",
    "report_every_after = 100\n",
    "\n",
    "print(\"number of epochs  : %s \" % number_of_epochs)\n",
    "print(\"batch size        : %s \" % BATCH_SIZE)\n",
    "print(\"report every after: %s steps\" % report_every_after)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(number_of_steps):\n",
    "        try:\n",
    "            _, _loss, _accuracy = sess.run([train_op, loss, accuracy])\n",
    "            if i % report_every_after == 0:\n",
    "                print('Batch No %s: accuracy: %s, loss: %s' % (i, _accuracy, _loss))\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            print(\"Done.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
