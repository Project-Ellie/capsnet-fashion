{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0-rc1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import mnist_tools\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t10k-images-idx3-ubyte.gz',\n",
       " 't10k-labels-idx1-ubyte.gz',\n",
       " 'train-labels-idx1-ubyte.gz',\n",
       " 'train-images-idx3-ubyte.gz']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"/var/ellie/data/mnist\"\n",
    "img_train = os.path.join(DATA_DIR, \"train-images-idx3-ubyte.gz\")\n",
    "lbl_train = os.path.join(DATA_DIR, \"train-labels-idx1-ubyte.gz\")\n",
    "img_test = os.path.join(DATA_DIR, \"t10k-images-idx3-ubyte.gz\")\n",
    "lbl_test = os.path.join(DATA_DIR, \"t10k-labels-idx1-ubyte.gz\")\n",
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scale(imgs, lbls):\n",
    "    return imgs / 256, lbls\n",
    "\n",
    "BATCH_SIZE=1000\n",
    "tr, te = mnist_tools.datasets(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext_1:0' shape=(?, 784) dtype=uint8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = tr.repeat().batch(BATCH_SIZE).make_one_shot_iterator().get_next()\n",
    "tr_images, tr_labels = train_iter\n",
    "tr_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1 154 253  90   0   0   0   0   0   0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    i,l = sess.run([tr_images, tr_labels])\n",
    "    print(i[0, 290:300])\n",
    "i.shape, l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dense( hidden ):\n",
    "    def _dense ( X, Y_ ):\n",
    "        #X=tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "        #Y_=tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "        X = tf.cast(X, tf.float32)\n",
    "        X = tf.subtract(tf.divide(X, tf.constant(256.0)), tf.constant(0.5))\n",
    "        W1 = tf.Variable(tf.random_normal(shape=[784, hidden]), dtype=tf.float32)\n",
    "        b1 = tf.Variable(tf.random_normal(shape=[hidden]), dtype=tf.float32)\n",
    "        a1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "        W2 = tf.Variable(tf.random_normal(shape=[hidden, 10]), dtype=tf.float32)\n",
    "        b2 = tf.Variable(tf.random_normal(shape=[10]), dtype=tf.float32)\n",
    "        logits = tf.matmul(a1, W2) + b2\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y_, logits=logits))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        accuracy=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), pred), dtype=tf.float32))\n",
    "        return (X, Y_, train_op, loss, accuracy)\n",
    "    return _dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y_, train_op, loss, accuracy = Dense(128)(tr_images, tr_labels)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of epochs  : 100 \n",
      "batch size        : 1000 \n",
      "report every after: 100 steps\n",
      "Batch No 0: accuracy: 0.136, loss: 142.01994\n",
      "Batch No 100: accuracy: 0.549, loss: 14.712262\n",
      "Batch No 200: accuracy: 0.724, loss: 8.19953\n",
      "Batch No 300: accuracy: 0.783, loss: 6.0909266\n",
      "Batch No 400: accuracy: 0.828, loss: 3.7552752\n",
      "Batch No 500: accuracy: 0.844, loss: 4.087698\n",
      "Batch No 600: accuracy: 0.847, loss: 3.7406902\n",
      "Batch No 700: accuracy: 0.858, loss: 2.3290784\n",
      "Batch No 800: accuracy: 0.872, loss: 2.8651116\n",
      "Batch No 900: accuracy: 0.881, loss: 2.6206524\n",
      "Batch No 1000: accuracy: 0.89, loss: 1.6419396\n",
      "Batch No 1100: accuracy: 0.892, loss: 2.1029036\n",
      "Batch No 1200: accuracy: 0.89, loss: 1.945506\n",
      "Batch No 1300: accuracy: 0.913, loss: 1.2520878\n",
      "Batch No 1400: accuracy: 0.914, loss: 1.7080908\n",
      "Batch No 1500: accuracy: 0.913, loss: 1.4830626\n",
      "Batch No 1600: accuracy: 0.928, loss: 0.97199607\n",
      "Batch No 1700: accuracy: 0.923, loss: 1.4526408\n",
      "Batch No 1800: accuracy: 0.923, loss: 1.2173816\n",
      "Batch No 1900: accuracy: 0.933, loss: 0.780058\n",
      "Batch No 2000: accuracy: 0.927, loss: 1.2792785\n",
      "Batch No 2100: accuracy: 0.928, loss: 1.0218985\n",
      "Batch No 2200: accuracy: 0.944, loss: 0.6361942\n",
      "Batch No 2300: accuracy: 0.926, loss: 1.1296167\n",
      "Batch No 2400: accuracy: 0.937, loss: 0.86777854\n",
      "Batch No 2500: accuracy: 0.95, loss: 0.5255028\n",
      "Batch No 2600: accuracy: 0.931, loss: 1.0075425\n",
      "Batch No 2700: accuracy: 0.943, loss: 0.7361117\n",
      "Batch No 2800: accuracy: 0.956, loss: 0.43953285\n",
      "Batch No 2900: accuracy: 0.933, loss: 0.91218126\n",
      "Batch No 3000: accuracy: 0.945, loss: 0.63349646\n",
      "Batch No 3100: accuracy: 0.957, loss: 0.36902443\n",
      "Batch No 3200: accuracy: 0.939, loss: 0.8237617\n",
      "Batch No 3300: accuracy: 0.949, loss: 0.55524796\n",
      "Batch No 3400: accuracy: 0.96, loss: 0.3116764\n",
      "Batch No 3500: accuracy: 0.945, loss: 0.73546106\n",
      "Batch No 3600: accuracy: 0.951, loss: 0.493759\n",
      "Batch No 3700: accuracy: 0.966, loss: 0.27030066\n",
      "Batch No 3800: accuracy: 0.953, loss: 0.66141343\n",
      "Batch No 3900: accuracy: 0.958, loss: 0.4466342\n",
      "Batch No 4000: accuracy: 0.973, loss: 0.2424169\n",
      "Batch No 4100: accuracy: 0.956, loss: 0.5907245\n",
      "Batch No 4200: accuracy: 0.959, loss: 0.40999418\n",
      "Batch No 4300: accuracy: 0.973, loss: 0.21526946\n",
      "Batch No 4400: accuracy: 0.961, loss: 0.5235437\n",
      "Batch No 4500: accuracy: 0.96, loss: 0.37789202\n",
      "Batch No 4600: accuracy: 0.976, loss: 0.19907199\n",
      "Batch No 4700: accuracy: 0.963, loss: 0.46796197\n",
      "Batch No 4800: accuracy: 0.963, loss: 0.3459278\n",
      "Batch No 4900: accuracy: 0.975, loss: 0.17689538\n",
      "Batch No 5000: accuracy: 0.966, loss: 0.4249203\n",
      "Batch No 5100: accuracy: 0.963, loss: 0.3151933\n",
      "Batch No 5200: accuracy: 0.978, loss: 0.15900357\n",
      "Batch No 5300: accuracy: 0.966, loss: 0.38274372\n",
      "Batch No 5400: accuracy: 0.966, loss: 0.28750682\n",
      "Batch No 5500: accuracy: 0.976, loss: 0.14666715\n",
      "Batch No 5600: accuracy: 0.97, loss: 0.34884664\n",
      "Batch No 5700: accuracy: 0.972, loss: 0.26492077\n",
      "Batch No 5800: accuracy: 0.977, loss: 0.13797332\n",
      "Batch No 5900: accuracy: 0.966, loss: 0.31682122\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 100\n",
    "number_of_steps = number_of_epochs * 60000 // BATCH_SIZE\n",
    "report_every_after = 100\n",
    "\n",
    "print(\"number of epochs  : %s \" % number_of_epochs)\n",
    "print(\"batch size        : %s \" % BATCH_SIZE)\n",
    "print(\"report every after: %s steps\" % report_every_after)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(number_of_steps):\n",
    "        try:\n",
    "            _, _loss, _accuracy = sess.run([train_op, loss, accuracy])\n",
    "            if i % report_every_after == 0:\n",
    "                print('Batch No %s: accuracy: %s, loss: %s' % (i, _accuracy, _loss))\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            print(\"Done.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
